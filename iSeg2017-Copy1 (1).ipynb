{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ghazala Khan\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import nibabel as nib\n",
    "import numpy as np\n",
    "#import tensorflow\n",
    "import keras\n",
    "\n",
    "# Fix random seed for reproducibility?\n",
    "# Better to follow the advice in Keras FAQ:\n",
    "#  \"How can I obtain reproducible results using Keras during development?\"\n",
    "seed = 7\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_classes = 3\n",
    "\n",
    "patience = 1\n",
    "model_filename = 'models/iSeg2017/outrun_step_{}.h5'\n",
    "csv_filename = 'log/iSeg2017/outrun_step_{}.cvs'\n",
    "\n",
    "nb_epoch = 20\n",
    "validation_split = 0.25\n",
    "\n",
    "class_mapper = {0 : 0, 10 : 0, 150 : 1, 250 : 2}\n",
    "class_mapper_inv = {0 : 0, 1 : 10, 2 : 150, 3 : 250}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# General utils for reading and saving data\n",
    "def get_filename(set_name, case_idx, input_name, loc='datasets') :\n",
    "    pattern = '{0}/iSeg2017/iSeg-2017-{1}/subject-{2}-{3}.hdr'\n",
    "    return pattern.format(loc, set_name, case_idx, input_name)\n",
    "\n",
    "def get_set_name(case_idx) :\n",
    "    return 'Training' if case_idx < 11 else 'Testing'\n",
    "\n",
    "def read_data(case_idx, input_name, loc='datasets') :\n",
    "    set_name = get_set_name(case_idx)\n",
    "\n",
    "    image_path = get_filename(set_name, case_idx, input_name, loc)\n",
    "\n",
    "    return nib.load(image_path)\n",
    "\n",
    "def read_vol(case_idx, input_name, loc='datasets') :\n",
    "    image_data = read_data(case_idx, input_name, loc)\n",
    "\n",
    "    return image_data.get_data()[:, :, :, 0]\n",
    "\n",
    "def save_vol(segmentation, case_idx, loc='results') :\n",
    "    set_name = get_set_name(case_idx)\n",
    "    input_image_data = read_data(case_idx, 'T1')\n",
    "\n",
    "    segmentation_vol = np.empty(input_image_data.shape)\n",
    "    segmentation_vol[:144, :192, :256, 0] = segmentation\n",
    "    \n",
    "    filename = get_filename(set_name, case_idx, 'label', loc)\n",
    "    nib.save(nib.analyze.AnalyzeImage(\n",
    "        segmentation_vol.astype('uint8'), input_image_data.affine), filename)\n",
    "\n",
    "\n",
    "# Data preparation utils\n",
    "from keras.utils import np_utils\n",
    "from sklearn.feature_extraction.image import extract_patches as sk_extract_patches\n",
    "\n",
    "def extract_patches(volume, patch_shape, extraction_step) :\n",
    "    patches = sk_extract_patches(\n",
    "        volume,\n",
    "        patch_shape=patch_shape,\n",
    "        extraction_step=extraction_step)\n",
    "\n",
    "    ndim = len(volume.shape)\n",
    "    npatches = np.prod(patches.shape[:ndim])\n",
    "    return patches.reshape((npatches, ) + patch_shape)\n",
    "\n",
    "def build_set(T1_vols, T2_vols, label_vols, extraction_step=(9, 9, 9)) :\n",
    "    patch_shape = (27, 27, 27)\n",
    "    label_selector = [slice(None)] + [slice(9, 18) for i in range(3)]\n",
    "\n",
    "    # Extract patches from input volumes and ground truth\n",
    "    x = np.zeros((0, 2, 27, 27, 27))\n",
    "    y = np.zeros((0, 9 * 9 * 9, num_classes))\n",
    "    for idx in range(len(T1_vols)) :\n",
    "        y_length = len(y)\n",
    "\n",
    "        label_patches = extract_patches(label_vols[idx], patch_shape, extraction_step)\n",
    "        label_patches = label_patches[label_selector]\n",
    "\n",
    "        # Select only those who are important for processing\n",
    "        valid_idxs = np.where(np.sum(label_patches, axis=(1, 2, 3)) != 0)\n",
    "\n",
    "        # Filtering extracted patches\n",
    "        label_patches = label_patches[valid_idxs]\n",
    "\n",
    "        x = np.vstack((x, np.zeros((len(label_patches), 2, 27, 27, 27))))\n",
    "        y = np.vstack((y, np.zeros((len(label_patches), 9 * 9 * 9, num_classes))))\n",
    "\n",
    "        for i in range(len(label_patches)) :\n",
    "            y[i+y_length, :, :] = np_utils.to_categorical(label_patches[i].flatten(), num_classes)\n",
    "\n",
    "        del label_patches\n",
    "\n",
    "        # Sampling strategy: reject samples which labels are only zeros\n",
    "        T1_train = extract_patches(T1_vols[idx], patch_shape, extraction_step)\n",
    "        x[y_length:, 0, :, :, :] = T1_train[valid_idxs]\n",
    "        del T1_train\n",
    "\n",
    "        # Sampling strategy: reject samples which labels are only zeros\n",
    "        T2_train = extract_patches(T2_vols[idx], patch_shape, extraction_step)\n",
    "        x[y_length:, 1, :, :, :] = T2_train[valid_idxs]\n",
    "        del T2_train\n",
    "    return x, y\n",
    "\n",
    "# Reconstruction utils\n",
    "import itertools\n",
    "\n",
    "def generate_indexes(patch_shape, expected_shape) :\n",
    "    ndims = len(patch_shape)\n",
    "\n",
    "    poss_shape = [patch_shape[i+1] * (expected_shape[i] // patch_shape[i+1]) for i in range(ndims-1)]\n",
    "\n",
    "    idxs = [range(patch_shape[i+1], poss_shape[i] - patch_shape[i+1], patch_shape[i+1]) for i in range(ndims-1)]\n",
    "\n",
    "    return itertools.product(*idxs)\n",
    "\n",
    "def reconstruct_volume(patches, expected_shape) :\n",
    "    patch_shape = patches.shape\n",
    "\n",
    "    assert len(patch_shape) - 1 == len(expected_shape)\n",
    "\n",
    "    reconstructed_img = np.zeros(expected_shape)\n",
    "\n",
    "    for count, coord in enumerate(generate_indexes(patch_shape, expected_shape)) :\n",
    "        selection = [slice(coord[i], coord[i] + patch_shape[i+1]) for i in range(len(coord))]\n",
    "        reconstructed_img[selection] = patches[count]\n",
    "\n",
    "    return reconstructed_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Input\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.layers.convolutional import Conv3D\n",
    "from keras.layers.convolutional import Cropping3D\n",
    "from keras.layers.core import Permute\n",
    "from keras.layers.core import Reshape\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model\n",
    "\n",
    "K.set_image_dim_ordering('th')\n",
    "\n",
    "# For understanding the architecture itself, I recommend checking the following article\n",
    "# Dolz, J. et al. 3D fully convolutional networks for subcortical segmentation in MRI :\n",
    "# A large-scale study. Neuroimage, 2017.\n",
    "def generate_model(num_classes) :\n",
    "    init_input = Input((2, 27, 27, 27))\n",
    "\n",
    "    x = Conv3D(5, kernel_size=(3, 3, 3))(init_input)\n",
    "    x = PReLU()(x)\n",
    "    x = Conv3D(5, kernel_size=(3, 3, 3))(x)\n",
    "    x = PReLU()(x)\n",
    "    x = Conv3D(5, kernel_size=(3, 3, 3))(x)\n",
    "    x = PReLU()(x)\n",
    "\n",
    "    y = Conv3D(10, kernel_size=(3, 3, 3))(x)\n",
    "    y = PReLU()(y)\n",
    "    y = Conv3D(10, kernel_size=(3, 3, 3))(y)\n",
    "    y = PReLU()(y)\n",
    "    y = Conv3D(10, kernel_size=(3, 3, 3))(y)\n",
    "    y = PReLU()(y)\n",
    "\n",
    "    z = Conv3D(15, kernel_size=(3, 3, 3))(y)\n",
    "    z = PReLU()(z)\n",
    "    z = Conv3D(15, kernel_size=(3, 3, 3))(z)\n",
    "    z = PReLU()(z)\n",
    "    z = Conv3D(15, kernel_size=(3, 3, 3))(z)\n",
    "    z = PReLU()(z)\n",
    "\n",
    "    x_crop = Cropping3D(cropping=((6, 6), (6, 6), (6, 6)))(x)\n",
    "    y_crop = Cropping3D(cropping=((3, 3), (3, 3), (3, 3)))(y)\n",
    "\n",
    "    concat = concatenate([x_crop, y_crop, z], axis=1)\n",
    "\n",
    "    fc = Conv3D(40, kernel_size=(1, 1, 1))(concat)\n",
    "    fc = PReLU()(fc)\n",
    "    fc = Conv3D(20, kernel_size=(1, 1, 1))(fc)\n",
    "    fc = PReLU()(fc)\n",
    "    fc = Conv3D(10, kernel_size=(1, 1, 1))(fc)\n",
    "    fc = PReLU()(fc)\n",
    "\n",
    "    pred = Conv3D(num_classes, kernel_size=(1, 1, 1))(fc)\n",
    "    pred = PReLU()(pred)\n",
    "    pred = Reshape((num_classes, 9 * 9 * 9))(pred)\n",
    "    pred = Permute((2, 1))(pred)\n",
    "    pred = Activation('softmax')(pred)\n",
    "\n",
    "    model = Model(inputs=init_input, outputs=pred)\n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy',\n",
    "        optimizer='adam',\n",
    "        metrics=['categorical_accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Initial segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "T1_vols = np.empty((10, 144, 192, 256))\n",
    "T2_vols = np.empty((10, 144, 192, 256))\n",
    "label_vols = np.empty((10, 144, 192, 256))\n",
    "for case_idx in range(1, 11) :\n",
    "    T1_vols[(case_idx - 1), :, :, :] = read_vol(case_idx, 'T1')\n",
    "    T2_vols[(case_idx - 1), :, :, :] = read_vol(case_idx, 'T2')\n",
    "    label_vols[(case_idx - 1), :, :, :] = read_vol(case_idx, 'label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Intensity normalisation (zero mean and unit variance)\n",
    "T1_mean = T1_vols.mean()\n",
    "T1_std = T1_vols.std()\n",
    "T1_vols = (T1_vols - T1_mean) / T1_std\n",
    "T2_mean = T2_vols.mean()\n",
    "T2_std = T2_vols.std()\n",
    "T2_vols = (T2_vols - T2_mean) / T2_std\n",
    "\n",
    "# Combine labels of BG and CSF\n",
    "for class_idx in class_mapper :\n",
    "    label_vols[label_vols == class_idx] = class_mapper[class_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train, y_train = build_set(T1_vols, T2_vols, label_vols, (6, 16, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 90 samples, validate on 10 samples\n",
      "Epoch 1/20\n",
      "90/90 [==============================] - 6s 65ms/step - loss: 1.0983 - categorical_accuracy: 0.5262 - val_loss: 1.0976 - val_categorical_accuracy: 0.6686\n",
      "Epoch 2/20\n",
      "90/90 [==============================] - 5s 54ms/step - loss: 1.0974 - categorical_accuracy: 0.6321 - val_loss: 1.0966 - val_categorical_accuracy: 0.6686\n",
      "Epoch 3/20\n",
      "90/90 [==============================] - 5s 54ms/step - loss: 1.0966 - categorical_accuracy: 0.6321 - val_loss: 1.0956 - val_categorical_accuracy: 0.6686\n",
      "Epoch 4/20\n",
      "90/90 [==============================] - 5s 54ms/step - loss: 1.0957 - categorical_accuracy: 0.6321 - val_loss: 1.0946 - val_categorical_accuracy: 0.6686\n",
      "Epoch 5/20\n",
      "90/90 [==============================] - 5s 54ms/step - loss: 1.0948 - categorical_accuracy: 0.6321 - val_loss: 1.0936 - val_categorical_accuracy: 0.6686\n",
      "Epoch 6/20\n",
      "90/90 [==============================] - 5s 61ms/step - loss: 1.0939 - categorical_accuracy: 0.6321 - val_loss: 1.0926 - val_categorical_accuracy: 0.6686\n",
      "Epoch 7/20\n",
      "90/90 [==============================] - 5s 57ms/step - loss: 1.0930 - categorical_accuracy: 0.6321 - val_loss: 1.0916 - val_categorical_accuracy: 0.6686\n",
      "Epoch 8/20\n",
      "90/90 [==============================] - 6s 62ms/step - loss: 1.0921 - categorical_accuracy: 0.6321 - val_loss: 1.0906 - val_categorical_accuracy: 0.6686\n",
      "Epoch 9/20\n",
      "90/90 [==============================] - 5s 61ms/step - loss: 1.0912 - categorical_accuracy: 0.6321 - val_loss: 1.0896 - val_categorical_accuracy: 0.6686\n",
      "Epoch 10/20\n",
      "90/90 [==============================] - 5s 57ms/step - loss: 1.0903 - categorical_accuracy: 0.6321 - val_loss: 1.0886 - val_categorical_accuracy: 0.6686\n",
      "Epoch 11/20\n",
      "90/90 [==============================] - 5s 61ms/step - loss: 1.0894 - categorical_accuracy: 0.6321 - val_loss: 1.0876 - val_categorical_accuracy: 0.6686\n",
      "Epoch 12/20\n",
      "90/90 [==============================] - 5s 60ms/step - loss: 1.0885 - categorical_accuracy: 0.6321 - val_loss: 1.0866 - val_categorical_accuracy: 0.6686\n",
      "Epoch 13/20\n",
      "90/90 [==============================] - 5s 60ms/step - loss: 1.0876 - categorical_accuracy: 0.6321 - val_loss: 1.0856 - val_categorical_accuracy: 0.6686\n",
      "Epoch 14/20\n",
      "90/90 [==============================] - 5s 59ms/step - loss: 1.0867 - categorical_accuracy: 0.6321 - val_loss: 1.0846 - val_categorical_accuracy: 0.6686\n",
      "Epoch 15/20\n",
      "90/90 [==============================] - 5s 59ms/step - loss: 1.0858 - categorical_accuracy: 0.6321 - val_loss: 1.0836 - val_categorical_accuracy: 0.6686\n",
      "Epoch 16/20\n",
      "90/90 [==============================] - 5s 59ms/step - loss: 1.0849 - categorical_accuracy: 0.6321 - val_loss: 1.0825 - val_categorical_accuracy: 0.6686\n",
      "Epoch 17/20\n",
      "90/90 [==============================] - 5s 60ms/step - loss: 1.0840 - categorical_accuracy: 0.6321 - val_loss: 1.0815 - val_categorical_accuracy: 0.6686\n",
      "Epoch 18/20\n",
      "90/90 [==============================] - 6s 61ms/step - loss: 1.0830 - categorical_accuracy: 0.6321 - val_loss: 1.0805 - val_categorical_accuracy: 0.6686\n",
      "Epoch 19/20\n",
      "90/90 [==============================] - 6s 63ms/step - loss: 1.0820 - categorical_accuracy: 0.6321 - val_loss: 1.0795 - val_categorical_accuracy: 0.6686\n",
      "Epoch 20/20\n",
      "90/90 [==============================] - 6s 61ms/step - loss: 1.0811 - categorical_accuracy: 0.6321 - val_loss: 1.0784 - val_categorical_accuracy: 0.6686\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23a8293c048>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "abc=generate_model(3)\n",
    "\n",
    "#What we have removed so far: images 4 -> 11\n",
    "#patch size that splits into training and test set was doubled: less precision\n",
    "#values from training set after 100\n",
    "x_train=x_train[:100]\n",
    "y_train=y_train[:100]\n",
    "\n",
    "abc.fit(x_train,y_train,verbose=1,validation_split=0.1,epochs=20)\n",
    "#How to use the above NN:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Configure callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import CSVLogger\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Early stopping for reducing over-fitting risk\n",
    "stopper = EarlyStopping(patience=patience)\n",
    "\n",
    "# Model checkpoint to save the training results\n",
    "checkpointer = ModelCheckpoint(\n",
    "    filepath=model_filename.format(1),\n",
    "    verbose=0,\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True)\n",
    "\n",
    "# CSVLogger to save the training results in a csv file\n",
    "csv_logger = CSVLogger(csv_filename.format(1), separator=';')\n",
    "\n",
    "callbacks = [checkpointer, csv_logger, stopper]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12690 samples, validate on 4231 samples\n",
      "Epoch 1/20\n",
      " - 785s - loss: 0.9239 - categorical_accuracy: 0.5531 - val_loss: 0.7795 - val_categorical_accuracy: 0.6649\n",
      "Epoch 2/20\n",
      " - 804s - loss: 0.7565 - categorical_accuracy: 0.6534 - val_loss: 0.7211 - val_categorical_accuracy: 0.6630\n",
      "Epoch 3/20\n",
      " - 802s - loss: 0.6996 - categorical_accuracy: 0.6549 - val_loss: 0.6603 - val_categorical_accuracy: 0.6737\n",
      "Epoch 4/20\n",
      " - 804s - loss: 0.6585 - categorical_accuracy: 0.6733 - val_loss: 0.6246 - val_categorical_accuracy: 0.7008\n",
      "Epoch 5/20\n",
      " - 796s - loss: 0.6296 - categorical_accuracy: 0.6940 - val_loss: 0.6028 - val_categorical_accuracy: 0.6900\n",
      "Epoch 6/20\n",
      " - 794s - loss: 0.6046 - categorical_accuracy: 0.7077 - val_loss: 0.5877 - val_categorical_accuracy: 0.6923\n",
      "Epoch 7/20\n",
      " - 804s - loss: 0.5842 - categorical_accuracy: 0.7206 - val_loss: 0.5608 - val_categorical_accuracy: 0.7372\n",
      "Epoch 8/20\n",
      " - 812s - loss: 0.5709 - categorical_accuracy: 0.7275 - val_loss: 0.5528 - val_categorical_accuracy: 0.7327\n",
      "Epoch 9/20\n",
      " - 851s - loss: 0.5598 - categorical_accuracy: 0.7341 - val_loss: 0.5477 - val_categorical_accuracy: 0.7284\n",
      "Epoch 10/20\n",
      " - 5835s - loss: 0.5520 - categorical_accuracy: 0.7383 - val_loss: 0.5425 - val_categorical_accuracy: 0.7312\n",
      "Epoch 11/20\n",
      " - 799s - loss: 0.5432 - categorical_accuracy: 0.7437 - val_loss: 0.5283 - val_categorical_accuracy: 0.7534\n",
      "Epoch 12/20\n",
      " - 845s - loss: 0.5353 - categorical_accuracy: 0.7489 - val_loss: 0.5261 - val_categorical_accuracy: 0.7599\n",
      "Epoch 13/20\n",
      " - 861s - loss: 0.5303 - categorical_accuracy: 0.7511 - val_loss: 0.5148 - val_categorical_accuracy: 0.7570\n",
      "Epoch 14/20\n",
      " - 858s - loss: 0.5227 - categorical_accuracy: 0.7558 - val_loss: 0.5061 - val_categorical_accuracy: 0.7658\n",
      "Epoch 15/20\n",
      " - 857s - loss: 0.5162 - categorical_accuracy: 0.7603 - val_loss: 0.5083 - val_categorical_accuracy: 0.7576\n"
     ]
    }
   ],
   "source": [
    "# Build model\n",
    "\n",
    "model = generate_model(num_classes)\n",
    "\n",
    "model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=nb_epoch,\n",
    "    validation_split=validation_split,\n",
    "    verbose=2,\n",
    "    callbacks=callbacks)\n",
    "\n",
    "# freeing space\n",
    "del x_train\n",
    "del y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train on 12690 samples, validate on 4231 samples\n",
    "Epoch 1/20\n",
    " - 785s - loss: 0.9239 - categorical_accuracy: 0.5531 - val_loss: 0.7795 - val_categorical_accuracy: 0.6649\n",
    "Epoch 2/20\n",
    " - 804s - loss: 0.7565 - categorical_accuracy: 0.6534 - val_loss: 0.7211 - val_categorical_accuracy: 0.6630\n",
    "Epoch 3/20\n",
    " - 802s - loss: 0.6996 - categorical_accuracy: 0.6549 - val_loss: 0.6603 - val_categorical_accuracy: 0.6737\n",
    "Epoch 4/20\n",
    " - 804s - loss: 0.6585 - categorical_accuracy: 0.6733 - val_loss: 0.6246 - val_categorical_accuracy: 0.7008\n",
    "Epoch 5/20\n",
    " - 796s - loss: 0.6296 - categorical_accuracy: 0.6940 - val_loss: 0.6028 - val_categorical_accuracy: 0.6900\n",
    "Epoch 6/20\n",
    " - 794s - loss: 0.6046 - categorical_accuracy: 0.7077 - val_loss: 0.5877 - val_categorical_accuracy: 0.6923\n",
    "Epoch 7/20\n",
    " - 804s - loss: 0.5842 - categorical_accuracy: 0.7206 - val_loss: 0.5608 - val_categorical_accuracy: 0.7372\n",
    "Epoch 8/20\n",
    " - 812s - loss: 0.5709 - categorical_accuracy: 0.7275 - val_loss: 0.5528 - val_categorical_accuracy: 0.7327\n",
    "Epoch 9/20\n",
    " - 851s - loss: 0.5598 - categorical_accuracy: 0.7341 - val_loss: 0.5477 - val_categorical_accuracy: 0.7284\n",
    "Epoch 10/20\n",
    " - 5835s - loss: 0.5520 - categorical_accuracy: 0.7383 - val_loss: 0.5425 - val_categorical_accuracy: 0.7312\n",
    "Epoch 11/20\n",
    " - 799s - loss: 0.5432 - categorical_accuracy: 0.7437 - val_loss: 0.5283 - val_categorical_accuracy: 0.7534\n",
    "Epoch 12/20\n",
    " - 845s - loss: 0.5353 - categorical_accuracy: 0.7489 - val_loss: 0.5261 - val_categorical_accuracy: 0.7599\n",
    "Epoch 13/20\n",
    " - 861s - loss: 0.5303 - categorical_accuracy: 0.7511 - val_loss: 0.5148 - val_categorical_accuracy: 0.7570\n",
    "Epoch 14/20\n",
    " - 858s - loss: 0.5227 - categorical_accuracy: 0.7558 - val_loss: 0.5061 - val_categorical_accuracy: 0.7658\n",
    "Epoch 15/20\n",
    " - 857s - loss: 0.5162 - categorical_accuracy: 0.7603 - val_loss: 0.5083 - val_categorical_accuracy: 0.7576"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12690 samples, validate on 4231 samples\n",
      "Epoch 1/20\n",
      " - 2945s - loss: 0.5372 - categorical_accuracy: 0.7358 - val_loss: 0.4076 - val_categorical_accuracy: 0.8119\n",
      "Epoch 2/20\n",
      " - 2978s - loss: 0.3781 - categorical_accuracy: 0.8286 - val_loss: 0.3377 - val_categorical_accuracy: 0.8493\n",
      "Epoch 3/20\n",
      " - 2982s - loss: 0.3280 - categorical_accuracy: 0.8540 - val_loss: 0.3111 - val_categorical_accuracy: 0.8639\n",
      "Epoch 4/20\n",
      " - 2989s - loss: 0.3017 - categorical_accuracy: 0.8665 - val_loss: 0.3081 - val_categorical_accuracy: 0.8626\n",
      "Epoch 5/20\n",
      " - 2979s - loss: 0.2742 - categorical_accuracy: 0.8795 - val_loss: 0.2818 - val_categorical_accuracy: 0.8744\n",
      "Epoch 6/20\n",
      " - 3088s - loss: 0.2615 - categorical_accuracy: 0.8854 - val_loss: 0.2582 - val_categorical_accuracy: 0.8862\n",
      "Epoch 7/20\n",
      " - 3168s - loss: 0.2437 - categorical_accuracy: 0.8938 - val_loss: 0.2598 - val_categorical_accuracy: 0.8869\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "'''#Training large data by breaking it into batches\n",
    "\n",
    "model = generate_model(num_classes)\n",
    "\n",
    "for e in range(10) :\n",
    "    print (\"epoch %d\" %e)\n",
    "    for step in range(10) :\n",
    "        x_train, train_y = LoadTrainBatch(32)  #batch_size = 32\n",
    "        x_train = np.asarray(x_train)\n",
    "        y_train = np.asarray(y_train) \n",
    "        for train_X , train_Y in zip(x_train ,train_y) :\n",
    "            train_X = train_X.reshape(1, row, col, 3)\n",
    "            model.fit(train_X, train_Y, nb_epoch=1, callbacks=callbacks,verbose=2,validation_split=validation_split)\n",
    "            \n",
    "del train_X            \n",
    "del train_Y\n",
    "'''\n",
    "# Build model\n",
    "\n",
    "model = generate_model(num_classes)\n",
    "\n",
    "model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=nb_epoch,\n",
    "    validation_split=validation_split,\n",
    "    verbose=2,\n",
    "    batch_size=15,\n",
    "    callbacks=callbacks) \n",
    "\n",
    "# freeing space\n",
    "del x_train\n",
    "del y_train "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def generate_model(num_classes) :\n",
    "    init_input = Input((2, 27, 27, 27))\n",
    "\n",
    "    x = Conv3D(5, kernel_size=(3, 3, 3))(init_input)\n",
    "    x = PReLU()(x)\n",
    "    x = Conv3D(5, kernel_size=(3, 3, 3))(x)\n",
    "    x = PReLU()(x)\n",
    "    x = Conv3D(5, kernel_size=(3, 3, 3))(x)\n",
    "    x = PReLU()(x)\n",
    "\n",
    "    y = Conv3D(10, kernel_size=(3, 3, 3))(x)\n",
    "    y = PReLU()(y)\n",
    "    y = Conv3D(10, kernel_size=(3, 3, 3))(y)\n",
    "    y = PReLU()(y)\n",
    "    y = Conv3D(10, kernel_size=(3, 3, 3))(y)\n",
    "    y = PReLU()(y)\n",
    "\n",
    "    z = Conv3D(15, kernel_size=(3, 3, 3))(y)\n",
    "    z = PReLU()(z)\n",
    "    z = Conv3D(15, kernel_size=(3, 3, 3))(z)\n",
    "    z = PReLU()(z)\n",
    "    z = Conv3D(15, kernel_size=(3, 3, 3))(z)\n",
    "    z = PReLU()(z)\n",
    "\n",
    "    x_crop = Cropping3D(cropping=((6, 6), (6, 6), (6, 6)))(x)\n",
    "    y_crop = Cropping3D(cropping=((3, 3), (3, 3), (3, 3)))(y)\n",
    "\n",
    "    concat = concatenate([x_crop, y_crop, z], axis=1)\n",
    "\n",
    "    fc = Conv3D(40, kernel_size=(1, 1, 1))(concat)\n",
    "    fc = PReLU()(fc)\n",
    "    fc = Conv3D(20, kernel_size=(1, 1, 1))(fc)\n",
    "    fc = PReLU()(fc)\n",
    "    fc = Conv3D(10, kernel_size=(1, 1, 1))(fc)\n",
    "    fc = PReLU()(fc)\n",
    "\n",
    "Train on 12690 samples, validate on 4231 samples\n",
    "Epoch 1/20\n",
    " - 2945s - loss: 0.5372 - categorical_accuracy: 0.7358 - val_loss: 0.4076 - val_categorical_accuracy: 0.8119\n",
    "Epoch 2/20\n",
    " - 2978s - loss: 0.3781 - categorical_accuracy: 0.8286 - val_loss: 0.3377 - val_categorical_accuracy: 0.8493\n",
    "Epoch 3/20\n",
    " - 2982s - loss: 0.3280 - categorical_accuracy: 0.8540 - val_loss: 0.3111 - val_categorical_accuracy: 0.8639\n",
    "Epoch 4/20\n",
    " - 2989s - loss: 0.3017 - categorical_accuracy: 0.8665 - val_loss: 0.3081 - val_categorical_accuracy: 0.8626\n",
    "Epoch 5/20\n",
    " - 2979s - loss: 0.2742 - categorical_accuracy: 0.8795 - val_loss: 0.2818 - val_categorical_accuracy: 0.8744\n",
    "Epoch 6/20\n",
    " - 3088s - loss: 0.2615 - categorical_accuracy: 0.8854 - val_loss: 0.2582 - val_categorical_accuracy: 0.8862\n",
    "Epoch 7/20\n",
    " - 3168s - loss: 0.2437 - categorical_accuracy: 0.8938 - val_loss: 0.2598 - val_categorical_accuracy: 0.8869"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 2, 27, 27, 27 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_1 (Conv3D)               (None, 5, 25, 25, 25 275         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_1 (PReLU)               (None, 5, 25, 25, 25 78125       conv3d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_2 (Conv3D)               (None, 5, 23, 23, 23 680         p_re_lu_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_2 (PReLU)               (None, 5, 23, 23, 23 60835       conv3d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_3 (Conv3D)               (None, 5, 21, 21, 21 680         p_re_lu_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_3 (PReLU)               (None, 5, 21, 21, 21 46305       conv3d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_4 (Conv3D)               (None, 10, 19, 19, 1 1360        p_re_lu_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_4 (PReLU)               (None, 10, 19, 19, 1 68590       conv3d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_5 (Conv3D)               (None, 10, 17, 17, 1 2710        p_re_lu_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_5 (PReLU)               (None, 10, 17, 17, 1 49130       conv3d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_6 (Conv3D)               (None, 10, 15, 15, 1 2710        p_re_lu_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_6 (PReLU)               (None, 10, 15, 15, 1 33750       conv3d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_7 (Conv3D)               (None, 15, 13, 13, 1 4065        p_re_lu_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_7 (PReLU)               (None, 15, 13, 13, 1 32955       conv3d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_8 (Conv3D)               (None, 15, 11, 11, 1 6090        p_re_lu_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_8 (PReLU)               (None, 15, 11, 11, 1 19965       conv3d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_9 (Conv3D)               (None, 15, 9, 9, 9)  6090        p_re_lu_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "cropping3d_1 (Cropping3D)       (None, 5, 9, 9, 9)   0           p_re_lu_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "cropping3d_2 (Cropping3D)       (None, 10, 9, 9, 9)  0           p_re_lu_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_9 (PReLU)               (None, 15, 9, 9, 9)  10935       conv3d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 30, 9, 9, 9)  0           cropping3d_1[0][0]               \n",
      "                                                                 cropping3d_2[0][0]               \n",
      "                                                                 p_re_lu_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_10 (Conv3D)              (None, 40, 9, 9, 9)  1240        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_10 (PReLU)              (None, 40, 9, 9, 9)  29160       conv3d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_11 (Conv3D)              (None, 20, 9, 9, 9)  820         p_re_lu_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_11 (PReLU)              (None, 20, 9, 9, 9)  14580       conv3d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_12 (Conv3D)              (None, 10, 9, 9, 9)  210         p_re_lu_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_12 (PReLU)              (None, 10, 9, 9, 9)  7290        conv3d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_13 (Conv3D)              (None, 3, 9, 9, 9)   33          p_re_lu_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_13 (PReLU)              (None, 3, 9, 9, 9)   2187        conv3d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 3, 729)       0           p_re_lu_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "permute_1 (Permute)             (None, 729, 3)       0           reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 729, 3)       0           permute_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 480,770\n",
      "Trainable params: 480,770\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            (None, 2, 27, 27, 27 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_92 (Conv3D)              (None, 1, 25, 25, 25 55          input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_92 (PReLU)              (None, 1, 25, 25, 25 15625       conv3d_92[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_93 (Conv3D)              (None, 1, 23, 23, 23 28          p_re_lu_92[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_93 (PReLU)              (None, 1, 23, 23, 23 12167       conv3d_93[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_94 (Conv3D)              (None, 1, 21, 21, 21 28          p_re_lu_93[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_94 (PReLU)              (None, 1, 21, 21, 21 9261        conv3d_94[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_95 (Conv3D)              (None, 1, 19, 19, 19 28          p_re_lu_94[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_95 (PReLU)              (None, 1, 19, 19, 19 6859        conv3d_95[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_96 (Conv3D)              (None, 1, 17, 17, 17 28          p_re_lu_95[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_96 (PReLU)              (None, 1, 17, 17, 17 4913        conv3d_96[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_97 (Conv3D)              (None, 1, 15, 15, 15 28          p_re_lu_96[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_97 (PReLU)              (None, 1, 15, 15, 15 3375        conv3d_97[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_98 (Conv3D)              (None, 1, 13, 13, 13 28          p_re_lu_97[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_98 (PReLU)              (None, 1, 13, 13, 13 2197        conv3d_98[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_99 (Conv3D)              (None, 1, 11, 11, 11 28          p_re_lu_98[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_99 (PReLU)              (None, 1, 11, 11, 11 1331        conv3d_99[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_100 (Conv3D)             (None, 1, 9, 9, 9)   28          p_re_lu_99[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "cropping3d_15 (Cropping3D)      (None, 1, 9, 9, 9)   0           p_re_lu_94[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "cropping3d_16 (Cropping3D)      (None, 1, 9, 9, 9)   0           p_re_lu_97[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_100 (PReLU)             (None, 1, 9, 9, 9)   729         conv3d_100[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 3, 9, 9, 9)   0           cropping3d_15[0][0]              \n",
      "                                                                 cropping3d_16[0][0]              \n",
      "                                                                 p_re_lu_100[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_101 (Conv3D)             (None, 4, 9, 9, 9)   16          concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_101 (PReLU)             (None, 4, 9, 9, 9)   2916        conv3d_101[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_102 (Conv3D)             (None, 2, 9, 9, 9)   10          p_re_lu_101[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_102 (PReLU)             (None, 2, 9, 9, 9)   1458        conv3d_102[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_103 (Conv3D)             (None, 1, 9, 9, 9)   3           p_re_lu_102[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_103 (PReLU)             (None, 1, 9, 9, 9)   729         conv3d_103[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_104 (Conv3D)             (None, 3, 9, 9, 9)   6           p_re_lu_103[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_104 (PReLU)             (None, 3, 9, 9, 9)   2187        conv3d_104[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "reshape_8 (Reshape)             (None, 3, 729)       0           p_re_lu_104[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "permute_8 (Permute)             (None, 729, 3)       0           reshape_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 729, 3)       0           permute_8[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 64,061\n",
      "Trainable params: 64,061\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________________________________________________________________________________________________\n",
    "Layer (type)                    Output Shape         Param #     Connected to                     \n",
    "==================================================================================================\n",
    "input_8 (InputLayer)            (None, 2, 27, 27, 27 0                                            \n",
    "__________________________________________________________________________________________________\n",
    "conv3d_92 (Conv3D)              (None, 1, 25, 25, 25 55          input_8[0][0]                    \n",
    "__________________________________________________________________________________________________\n",
    "p_re_lu_92 (PReLU)              (None, 1, 25, 25, 25 15625       conv3d_92[0][0]                  \n",
    "__________________________________________________________________________________________________\n",
    "conv3d_93 (Conv3D)              (None, 1, 23, 23, 23 28          p_re_lu_92[0][0]                 \n",
    "__________________________________________________________________________________________________\n",
    "p_re_lu_93 (PReLU)              (None, 1, 23, 23, 23 12167       conv3d_93[0][0]                  \n",
    "__________________________________________________________________________________________________\n",
    "conv3d_94 (Conv3D)              (None, 1, 21, 21, 21 28          p_re_lu_93[0][0]                 \n",
    "__________________________________________________________________________________________________\n",
    "p_re_lu_94 (PReLU)              (None, 1, 21, 21, 21 9261        conv3d_94[0][0]                  \n",
    "__________________________________________________________________________________________________\n",
    "conv3d_95 (Conv3D)              (None, 1, 19, 19, 19 28          p_re_lu_94[0][0]                 \n",
    "__________________________________________________________________________________________________\n",
    "p_re_lu_95 (PReLU)              (None, 1, 19, 19, 19 6859        conv3d_95[0][0]                  \n",
    "__________________________________________________________________________________________________\n",
    "conv3d_96 (Conv3D)              (None, 1, 17, 17, 17 28          p_re_lu_95[0][0]                 \n",
    "__________________________________________________________________________________________________\n",
    "p_re_lu_96 (PReLU)              (None, 1, 17, 17, 17 4913        conv3d_96[0][0]                  \n",
    "__________________________________________________________________________________________________\n",
    "conv3d_97 (Conv3D)              (None, 1, 15, 15, 15 28          p_re_lu_96[0][0]                 \n",
    "__________________________________________________________________________________________________\n",
    "p_re_lu_97 (PReLU)              (None, 1, 15, 15, 15 3375        conv3d_97[0][0]                  \n",
    "__________________________________________________________________________________________________\n",
    "conv3d_98 (Conv3D)              (None, 1, 13, 13, 13 28          p_re_lu_97[0][0]                 \n",
    "__________________________________________________________________________________________________\n",
    "p_re_lu_98 (PReLU)              (None, 1, 13, 13, 13 2197        conv3d_98[0][0]                  \n",
    "__________________________________________________________________________________________________\n",
    "conv3d_99 (Conv3D)              (None, 1, 11, 11, 11 28          p_re_lu_98[0][0]                 \n",
    "__________________________________________________________________________________________________\n",
    "p_re_lu_99 (PReLU)              (None, 1, 11, 11, 11 1331        conv3d_99[0][0]                  \n",
    "__________________________________________________________________________________________________\n",
    "conv3d_100 (Conv3D)             (None, 1, 9, 9, 9)   28          p_re_lu_99[0][0]                 \n",
    "__________________________________________________________________________________________________\n",
    "cropping3d_15 (Cropping3D)      (None, 1, 9, 9, 9)   0           p_re_lu_94[0][0]                 \n",
    "__________________________________________________________________________________________________\n",
    "cropping3d_16 (Cropping3D)      (None, 1, 9, 9, 9)   0           p_re_lu_97[0][0]                 \n",
    "__________________________________________________________________________________________________\n",
    "p_re_lu_100 (PReLU)             (None, 1, 9, 9, 9)   729         conv3d_100[0][0]                 \n",
    "__________________________________________________________________________________________________\n",
    "concatenate_8 (Concatenate)     (None, 3, 9, 9, 9)   0           cropping3d_15[0][0]              \n",
    "                                                                 cropping3d_16[0][0]              \n",
    "                                                                 p_re_lu_100[0][0]                \n",
    "__________________________________________________________________________________________________\n",
    "conv3d_101 (Conv3D)             (None, 4, 9, 9, 9)   16          concatenate_8[0][0]              \n",
    "__________________________________________________________________________________________________\n",
    "p_re_lu_101 (PReLU)             (None, 4, 9, 9, 9)   2916        conv3d_101[0][0]                 \n",
    "__________________________________________________________________________________________________\n",
    "conv3d_102 (Conv3D)             (None, 2, 9, 9, 9)   10          p_re_lu_101[0][0]                \n",
    "__________________________________________________________________________________________________\n",
    "p_re_lu_102 (PReLU)             (None, 2, 9, 9, 9)   1458        conv3d_102[0][0]                 \n",
    "__________________________________________________________________________________________________\n",
    "conv3d_103 (Conv3D)             (None, 1, 9, 9, 9)   3           p_re_lu_102[0][0]                \n",
    "__________________________________________________________________________________________________\n",
    "p_re_lu_103 (PReLU)             (None, 1, 9, 9, 9)   729         conv3d_103[0][0]                 \n",
    "__________________________________________________________________________________________________\n",
    "conv3d_104 (Conv3D)             (None, 3, 9, 9, 9)   6           p_re_lu_103[0][0]                \n",
    "__________________________________________________________________________________________________\n",
    "p_re_lu_104 (PReLU)             (None, 3, 9, 9, 9)   2187        conv3d_104[0][0]                 \n",
    "__________________________________________________________________________________________________\n",
    "reshape_8 (Reshape)             (None, 3, 729)       0           p_re_lu_104[0][0]                \n",
    "__________________________________________________________________________________________________\n",
    "permute_8 (Permute)             (None, 729, 3)       0           reshape_8[0][0]                  \n",
    "__________________________________________________________________________________________________\n",
    "activation_8 (Activation)       (None, 729, 3)       0           permute_8[0][0]                  \n",
    "==================================================================================================\n",
    "Total params: 64,061\n",
    "Trainable params: 64,061\n",
    "Non-trainable params: 0\n",
    "__________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "# Load best model\n",
    "model = generate_model(num_classes)\n",
    "model.load_weights(model_filename.format(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished segmentation of case # 11\n",
      "Finished segmentation of case # 12\n",
      "Finished segmentation of case # 13\n",
      "Finished segmentation of case # 14\n",
      "Finished segmentation of case # 15\n",
      "Finished segmentation of case # 16\n",
      "Finished segmentation of case # 17\n",
      "Finished segmentation of case # 18\n",
      "Finished segmentation of case # 19\n",
      "Finished segmentation of case # 20\n",
      "Finished segmentation of case # 21\n",
      "Finished segmentation of case # 22\n",
      "Finished segmentation of case # 23\n",
      "Done with Step 1\n"
     ]
    }
   ],
   "source": [
    "for case_idx in range(11, 24) :\n",
    "    T1_test_vol = read_vol(case_idx, 'T1')[:144, :192, :256]\n",
    "    T2_test_vol = read_vol(case_idx, 'T2')[:144, :192, :256]\n",
    "    \n",
    "    x_test = np.zeros((6916, 2, 27, 27, 27))\n",
    "    x_test[:, 0, :, :, :] = extract_patches(T1_test_vol, patch_shape=(27, 27, 27), extraction_step=(9, 9, 9))\n",
    "    x_test[:, 1, :, :, :] = extract_patches(T2_test_vol, patch_shape=(27, 27, 27), extraction_step=(9, 9, 9))\n",
    "    \n",
    "    x_test[:, 0, :, :, :] = (x_test[:, 0, :, :, :] - T1_mean) / T1_std\n",
    "    x_test[:, 1, :, :, :] = (x_test[:, 1, :, :, :] - T2_mean) / T2_std\n",
    "\n",
    "    pred = model.predict(x_test, verbose=2)\n",
    "    pred_classes = np.argmax(pred, axis=2)\n",
    "    pred_classes = pred_classes.reshape((len(pred_classes), 9, 9, 9))\n",
    "    segmentation = reconstruct_volume(pred_classes, (144, 192, 256))\n",
    "    \n",
    "    csf = np.logical_and(segmentation == 0, T1_test_vol != 0)\n",
    "    segmentation[segmentation == 2] = 250\n",
    "    segmentation[segmentation == 1] = 150\n",
    "    segmentation[csf] = 10\n",
    "    \n",
    "    save_vol(segmentation, case_idx)\n",
    "    \n",
    "    print( \"Finished segmentation of case # {}\" .format(case_idx)) #\n",
    "\n",
    "print( \"Done with Step 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Pseudo-labelling step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sure = range(0, 10)\n",
    "unsure = range(11, 23)\n",
    "\n",
    "T1_vols = np.empty((23, 144, 192, 256))\n",
    "T2_vols = np.empty((23, 144, 192, 256))\n",
    "label_vols = np.empty((23, 144, 192, 256))\n",
    "for case_idx in range(1, 24) :\n",
    "    loc = 'datasets' if case_idx < 11 else 'results'\n",
    "\n",
    "    T1_vols[(case_idx - 1), :, :, :] = read_vol(case_idx, 'T1')[:144, :192, :256]\n",
    "    T2_vols[(case_idx - 1), :, :, :] = read_vol(case_idx, 'T2')[:144, :192, :256]\n",
    "    label_vols[(case_idx - 1), :, :, :] = read_vol(case_idx, 'label', loc)[:144, :192, :256]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Intensity normalisation (zero mean and unit variance)\n",
    "T1_mean = T1_vols.mean()\n",
    "T1_std = T1_vols.std()\n",
    "T1_vols = (T1_vols - T1_mean) / T1_std\n",
    "T2_mean = T2_vols.mean()\n",
    "T2_std = T2_vols.std()\n",
    "T2_vols = (T2_vols - T2_mean) / T2_std\n",
    "\n",
    "# Combine labels of BG and CSF\n",
    "for class_idx in class_mapper :\n",
    "    label_vols[label_vols == class_idx] = class_mapper[class_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "x_sure, y_sure = build_set(T1_vols[sure], T2_vols[sure], label_vols[sure], (12,32,12))\n",
    "x_unsure, y_unsure = build_set(T1_vols[unsure], T2_vols[unsure], label_vols[unsure])\n",
    "\n",
    "#x_train=x_train[:100]\n",
    "#y_train=y_train[:100]\n",
    "\n",
    "x_train = np.vstack((x_sure, x_unsure))\n",
    "y_train = np.vstack((y_sure, y_unsure))\n",
    "\n",
    "del x_sure\n",
    "del x_unsure\n",
    "del y_sure\n",
    "del y_unsure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Configure callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import CSVLogger\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Early stopping for reducing over-fitting risk\n",
    "stopper = EarlyStopping(patience=patience)\n",
    "\n",
    "# Model checkpoint to save the training results\n",
    "checkpointer = ModelCheckpoint(\n",
    "    filepath=model_filename.format(2),\n",
    "    verbose=0,\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True)\n",
    "\n",
    "# CSVLogger to save the training results in a csv file\n",
    "csv_logger = CSVLogger(csv_filename.format(2), separator=';')\n",
    "\n",
    "callbacks = [checkpointer, csv_logger, stopper]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13983 samples, validate on 4662 samples\n",
      "Epoch 1/20\n",
      " - 845s - loss: 0.9262 - categorical_accuracy: 0.5298 - val_loss: 0.7076 - val_categorical_accuracy: 0.6720\n",
      "Epoch 2/20\n",
      " - 862s - loss: 0.7001 - categorical_accuracy: 0.6453 - val_loss: 0.6428 - val_categorical_accuracy: 0.6802\n",
      "Epoch 3/20\n",
      " - 861s - loss: 0.6492 - categorical_accuracy: 0.6766 - val_loss: 0.6083 - val_categorical_accuracy: 0.7065\n",
      "Epoch 4/20\n",
      " - 862s - loss: 0.5927 - categorical_accuracy: 0.7192 - val_loss: 0.5290 - val_categorical_accuracy: 0.7628\n",
      "Epoch 5/20\n",
      " - 862s - loss: 0.5018 - categorical_accuracy: 0.7797 - val_loss: 0.4419 - val_categorical_accuracy: 0.8181\n",
      "Epoch 6/20\n",
      " - 859s - loss: 0.4466 - categorical_accuracy: 0.8102 - val_loss: 0.4190 - val_categorical_accuracy: 0.8216\n",
      "Epoch 7/20\n",
      " - 830s - loss: 0.4199 - categorical_accuracy: 0.8222 - val_loss: 0.3892 - val_categorical_accuracy: 0.8340\n",
      "Epoch 8/20\n",
      " - 834s - loss: 0.4000 - categorical_accuracy: 0.8312 - val_loss: 0.3747 - val_categorical_accuracy: 0.8490\n",
      "Epoch 9/20\n",
      " - 842s - loss: 0.3860 - categorical_accuracy: 0.8378 - val_loss: 0.3521 - val_categorical_accuracy: 0.8616\n",
      "Epoch 10/20\n",
      " - 835s - loss: 0.3740 - categorical_accuracy: 0.8432 - val_loss: 0.3317 - val_categorical_accuracy: 0.8649\n",
      "Epoch 11/20\n",
      " - 837s - loss: 0.3649 - categorical_accuracy: 0.8468 - val_loss: 0.3864 - val_categorical_accuracy: 0.8322\n"
     ]
    }
   ],
   "source": [
    "# Build model\n",
    "model = generate_model(num_classes)\n",
    "\n",
    "model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=nb_epoch,\n",
    "    validation_split=validation_split,\n",
    "    verbose=2,\n",
    "    callbacks=callbacks)\n",
    "\n",
    "# freeing space\n",
    "del x_train\n",
    "del y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.5 Results \n",
    "Train on 13983 samples, validate on 4662 samples\n",
    "Epoch 1/20\n",
    " - 845s - loss: 0.9262 - categorical_accuracy: 0.5298 - val_loss: 0.7076 - val_categorical_accuracy: 0.6720\n",
    "Epoch 2/20\n",
    " - 862s - loss: 0.7001 - categorical_accuracy: 0.6453 - val_loss: 0.6428 - val_categorical_accuracy: 0.6802\n",
    "Epoch 3/20\n",
    " - 861s - loss: 0.6492 - categorical_accuracy: 0.6766 - val_loss: 0.6083 - val_categorical_accuracy: 0.7065\n",
    "Epoch 4/20\n",
    " - 862s - loss: 0.5927 - categorical_accuracy: 0.7192 - val_loss: 0.5290 - val_categorical_accuracy: 0.7628\n",
    "Epoch 5/20\n",
    " - 862s - loss: 0.5018 - categorical_accuracy: 0.7797 - val_loss: 0.4419 - val_categorical_accuracy: 0.8181\n",
    "Epoch 6/20\n",
    " - 859s - loss: 0.4466 - categorical_accuracy: 0.8102 - val_loss: 0.4190 - val_categorical_accuracy: 0.8216\n",
    "Epoch 7/20\n",
    " - 830s - loss: 0.4199 - categorical_accuracy: 0.8222 - val_loss: 0.3892 - val_categorical_accuracy: 0.8340\n",
    "Epoch 8/20\n",
    " - 834s - loss: 0.4000 - categorical_accuracy: 0.8312 - val_loss: 0.3747 - val_categorical_accuracy: 0.8490\n",
    "Epoch 9/20\n",
    " - 842s - loss: 0.3860 - categorical_accuracy: 0.8378 - val_loss: 0.3521 - val_categorical_accuracy: 0.8616\n",
    "Epoch 10/20\n",
    " - 835s - loss: 0.3740 - categorical_accuracy: 0.8432 - val_loss: 0.3317 - val_categorical_accuracy: 0.8649\n",
    "Epoch 11/20\n",
    " - 837s - loss: 0.3649 - categorical_accuracy: 0.8468 - val_loss: 0.3864 - val_categorical_accuracy: 0.8322"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13845 samples, validate on 4615 samples\n",
      "Epoch 1/20\n",
      " - 3445s - loss: 0.5010 - categorical_accuracy: 0.7540 - val_loss: 0.4341 - val_categorical_accuracy: 0.7892\n",
      "Epoch 2/20\n",
      " - 3412s - loss: 0.3368 - categorical_accuracy: 0.8483 - val_loss: 0.2886 - val_categorical_accuracy: 0.8730\n",
      "Epoch 3/20\n",
      " - 3418s - loss: 0.2813 - categorical_accuracy: 0.8776 - val_loss: 0.2503 - val_categorical_accuracy: 0.8913\n",
      "Epoch 4/20\n",
      " - 3406s - loss: 0.2426 - categorical_accuracy: 0.8957 - val_loss: 0.2463 - val_categorical_accuracy: 0.8939\n",
      "Epoch 5/20\n",
      " - 3395s - loss: 0.2182 - categorical_accuracy: 0.9065 - val_loss: 0.2161 - val_categorical_accuracy: 0.9064\n",
      "Epoch 6/20\n",
      " - 3400s - loss: 0.2037 - categorical_accuracy: 0.9129 - val_loss: 0.1802 - val_categorical_accuracy: 0.9229\n",
      "Epoch 7/20\n",
      " - 3395s - loss: 0.1968 - categorical_accuracy: 0.9160 - val_loss: 0.1776 - val_categorical_accuracy: 0.9244\n",
      "Epoch 8/20\n",
      " - 3359s - loss: 0.1861 - categorical_accuracy: 0.9206 - val_loss: 0.1680 - val_categorical_accuracy: 0.9279\n",
      "Epoch 9/20\n",
      " - 3353s - loss: 0.1794 - categorical_accuracy: 0.9236 - val_loss: 0.1614 - val_categorical_accuracy: 0.9305\n",
      "Epoch 10/20\n",
      " - 3348s - loss: 0.1741 - categorical_accuracy: 0.9259 - val_loss: 0.1613 - val_categorical_accuracy: 0.9309\n",
      "Epoch 11/20\n",
      " - 3347s - loss: 0.1710 - categorical_accuracy: 0.9273 - val_loss: 0.1643 - val_categorical_accuracy: 0.9302\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'flatten'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-a8ba3f8bc45a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mpreds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpreds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'flatten'"
     ]
    }
   ],
   "source": [
    "# Build model\n",
    "model = generate_model(num_classes)\n",
    "\n",
    "model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=nb_epoch,\n",
    "    validation_split=validation_split,\n",
    "    verbose=2,\n",
    "    batch_size=32,\n",
    "    callbacks=callbacks)\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "preds=np.flatten(model.predict(x_train))\n",
    "y_train=np.flatten(y_train)\n",
    "print(classification_report(y_train,preds))\n",
    "\n",
    "\n",
    "# freeing space\n",
    "del x_train\n",
    "del y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.5 results : \n",
    "\n",
    "Train on 13845 samples, validate on 4615 samples\n",
    "Epoch 1/20\n",
    " - 3445s - loss: 0.5010 - categorical_accuracy: 0.7540 - val_loss: 0.4341 - val_categorical_accuracy: 0.7892\n",
    "Epoch 2/20\n",
    " - 3412s - loss: 0.3368 - categorical_accuracy: 0.8483 - val_loss: 0.2886 - val_categorical_accuracy: 0.8730\n",
    "Epoch 3/20\n",
    " - 3418s - loss: 0.2813 - categorical_accuracy: 0.8776 - val_loss: 0.2503 - val_categorical_accuracy: 0.8913\n",
    "Epoch 4/20\n",
    " - 3406s - loss: 0.2426 - categorical_accuracy: 0.8957 - val_loss: 0.2463 - val_categorical_accuracy: 0.8939\n",
    "Epoch 5/20\n",
    " - 3395s - loss: 0.2182 - categorical_accuracy: 0.9065 - val_loss: 0.2161 - val_categorical_accuracy: 0.9064\n",
    "Epoch 6/20\n",
    " - 3400s - loss: 0.2037 - categorical_accuracy: 0.9129 - val_loss: 0.1802 - val_categorical_accuracy: 0.9229\n",
    "Epoch 7/20\n",
    " - 3395s - loss: 0.1968 - categorical_accuracy: 0.9160 - val_loss: 0.1776 - val_categorical_accuracy: 0.9244\n",
    "Epoch 8/20\n",
    " - 3359s - loss: 0.1861 - categorical_accuracy: 0.9206 - val_loss: 0.1680 - val_categorical_accuracy: 0.9279\n",
    "Epoch 9/20\n",
    " - 3353s - loss: 0.1794 - categorical_accuracy: 0.9236 - val_loss: 0.1614 - val_categorical_accuracy: 0.9305\n",
    "Epoch 10/20\n",
    " - 3348s - loss: 0.1741 - categorical_accuracy: 0.9259 - val_loss: 0.1613 - val_categorical_accuracy: 0.9309\n",
    "Epoch 11/20\n",
    " - 3347s - loss: 0.1710 - categorical_accuracy: 0.9273 - val_loss: 0.1643 - val_categorical_accuracy: 0.9302"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.72      1.00      0.84  26914680\n",
      "          1       1.00      0.21      0.35  13457340\n",
      "\n",
      "avg / total       0.81      0.74      0.67  40372020\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "preds=np.ndarray.flatten(model.predict(x_train))\n",
    "#preds=preds.astype(float)\n",
    "y_train=np.ndarray.flatten(y_train)\n",
    "\n",
    "y_train=(y_train.astype(int)).astype(str)\n",
    "preds=(preds.astype(int)).astype(str)\n",
    "print(classification_report(y_train,preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1' '0' '0' '1' '0' '0' '1' '0' '0' '1']\n",
      "['1' '0' '0' '1' '0' '0' '1' '0' '0' '1']\n"
     ]
    }
   ],
   "source": [
    "print(y_train[:10])\n",
    "\n",
    "print(preds[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40372020,)\n",
      "(40372020,)\n"
     ]
    }
   ],
   "source": [
    "#test_eval = model.evaluate(y_train,preds, verbose=0)\n",
    "print(y_train.shape)\n",
    "print(preds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report \n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "preds=np.ndarray.flatten(model.predict(x_train))\n",
    "\n",
    "#preds=preds.astype(float)\n",
    "y_train=np.ndarray.flatten(y_train)\n",
    "\n",
    "y_train=(y_train.astype(int)).astype(str)\n",
    "\n",
    "preds=(preds.astype(int)).astype(str)\n",
    "\n",
    "print(classification_report(y_train,preds))\n",
    "\n",
    "\n",
    "\n",
    "         precision    recall  f1-score   support\n",
    "\n",
    "          0       0.72      1.00      0.84  26914680\n",
    "          1       1.00      0.21      0.35  13457340\n",
    "\n",
    "avg / total       0.81      0.74      0.67  40372020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Clasification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "# Load best model\n",
    "model = generate_model(num_classes)\n",
    "model.load_weights(model_filename.format(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished segmentation of case # 11\n",
      "Finished segmentation of case # 12\n",
      "Finished segmentation of case # 13\n",
      "Finished segmentation of case # 14\n",
      "Finished segmentation of case # 15\n",
      "Finished segmentation of case # 16\n",
      "Finished segmentation of case # 17\n",
      "Finished segmentation of case # 18\n",
      "Finished segmentation of case # 19\n",
      "Finished segmentation of case # 20\n",
      "Finished segmentation of case # 21\n",
      "Finished segmentation of case # 22\n",
      "Finished segmentation of case # 23\n",
      "Done with Step 2\n"
     ]
    }
   ],
   "source": [
    "for case_idx in range(11, 24) :\n",
    "    T1_test_vol = read_vol(case_idx, 'T1')[:144, :192, :256]\n",
    "    T2_test_vol = read_vol(case_idx, 'T2')[:144, :192, :256]\n",
    "    \n",
    "    x_test = np.zeros((6916, 2, 27, 27, 27))\n",
    "    x_test[:, 0, :, :, :] = extract_patches(T1_test_vol, patch_shape=(27, 27, 27), extraction_step=(9, 9, 9))\n",
    "    x_test[:, 1, :, :, :] = extract_patches(T2_test_vol, patch_shape=(27, 27, 27), extraction_step=(9, 9, 9))\n",
    "    \n",
    "    x_test[:, 0, :, :, :] = (x_test[:, 0, :, :, :] - T1_mean) / T1_std\n",
    "    x_test[:, 1, :, :, :] = (x_test[:, 1, :, :, :] - T2_mean) / T2_std\n",
    "\n",
    "    pred = model.predict(x_test, verbose=2)\n",
    "    pred_classes = np.argmax(pred, axis=2)\n",
    "    pred_classes = pred_classes.reshape((len(pred_classes), 9, 9, 9))\n",
    "    segmentation = reconstruct_volume(pred_classes, (144, 192, 256))\n",
    "    \n",
    "    csf = np.logical_and(segmentation == 0, T1_test_vol != 0)\n",
    "    segmentation[segmentation == 2] = 250\n",
    "    segmentation[segmentation == 1] = 150\n",
    "    segmentation[csf] = 10\n",
    "    \n",
    "    save_vol(segmentation, case_idx, 'refined-results')\n",
    "    \n",
    "    print( \"Finished segmentation of case # {}\".format(case_idx))\n",
    "\n",
    "print( \"Done with Step 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Skip to content\n",
    " \n",
    "Search or jump to…\n",
    "\n",
    "Pull requests\n",
    "Issues\n",
    "Marketplace\n",
    "Explore\n",
    " @GKKhan21 Sign out\n",
    "0\n",
    "0 0 Rickey985/ISeg\n",
    " Code  Issues 0  Pull requests 0  Projects 0  Wiki  Insights\n",
    "ISeg/Data Loader2.py\n",
    "bf46afd  2 hours ago\n",
    "@Rickey985 Rickey985 Add files via upload\n",
    "     \n",
    "241 lines (183 sloc)  7.8 KB\"\"\"\n",
    "\n",
    "\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "\n",
    "# Fix random seed for reproducibility?\n",
    "# Better to follow the advice in Keras FAQ:\n",
    "#  \"How can I obtain reproducible results using Keras during development?\"\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "num_classes = 3\n",
    "\n",
    "patience = 1\n",
    "model_filename = 'models/iSeg2017/outrun_step_{}.h5'\n",
    "csv_filename = 'log/iSeg2017/outrun_step_{}.cvs'\n",
    "\n",
    "nb_epoch = 20\n",
    "validation_split = 0.25\n",
    "\n",
    "class_mapper = {0 : 0, 10 : 0, 150 : 1, 250 : 2}\n",
    "class_mapper_inv = {0 : 0, 1 : 10, 2 : 150, 3 : 250}\n",
    "\n",
    "\n",
    "# General utils for reading and saving data\n",
    "def get_filename(set_name, case_idx, input_name, loc='datasets'):\n",
    "    pattern = '{0}/iSeg2017/iSeg-2017-{1}/subject-{2}-{3}.hdr'\n",
    "    return pattern.format(loc, set_name, case_idx, input_name)\n",
    "def get_set_name(case_idx):\n",
    "    return 'Training' if case_idx < 4 else 'Testing'\n",
    "def read_data(case_idx, input_name, loc='datasets'):\n",
    "    set_name = get_set_name(case_idx)\n",
    "    image_path = get_filename(set_name, case_idx, input_name, loc)\n",
    "    return nib.load(image_path)\n",
    "def read_vol(case_idx, input_name, loc='datasets'):\n",
    "    image_data = read_data(case_idx, input_name, loc)\n",
    "    return image_data.get_data()[:, :, :, 0]\n",
    "\n",
    "def save_vol(segmentation, case_idx, loc='results'):\n",
    "    set_name = get_set_name(case_idx)\n",
    "    input_image_data = read_data(case_idx, 'T1')\n",
    "    segmentation_vol = np.empty(input_image_data.shape)\n",
    "    segmentation_vol[:144, :192, :256, 0] = segmentation\n",
    "    filename = get_filename(set_name, case_idx, 'label', loc)\n",
    "    nib.save(nib.analyze.AnalyzeImage(\n",
    "        segmentation_vol.astype('uint8'), input_image_data.affine), filename)\n",
    "\n",
    "\n",
    "# Data preparation utils\n",
    "from keras.utils import np_utils\n",
    "from sklearn.feature_extraction.image import extract_patches as sk_extract_patches\n",
    "\n",
    "\n",
    "def extract_patches(volume, patch_shape, extraction_step):\n",
    "    patches = sk_extract_patches(\n",
    "        volume,\n",
    "        patch_shape=patch_shape,\n",
    "        extraction_step=extraction_step)\n",
    "\n",
    "    ndim = len(volume.shape)\n",
    "    npatches = np.prod(patches.shape[:ndim])\n",
    "    return patches.reshape((npatches,) + patch_shape)\n",
    "\n",
    "\n",
    "def build_set(T1_vols, T2_vols, label_vols, extraction_step):\n",
    "    patch_shape = (27, 27, 27)\n",
    "    label_selector = [slice(None)] + [slice(9, 18) for i in range(3)]\n",
    "\n",
    "    # Extract patches from input volumes and ground truth\n",
    "    x = np.zeros((0, 2, 27, 27, 27))\n",
    "    y = np.zeros((0, 9 * 9 * 9, num_classes))\n",
    "    for idx in range(len(T1_vols)):\n",
    "        y_length = len(y)\n",
    "\n",
    "        label_patches = extract_patches(label_vols[idx], patch_shape, extraction_step)\n",
    "        label_patches = label_patches[label_selector]\n",
    "\n",
    "        # Select only those who are important for processing\n",
    "        valid_idxs = np.where(np.sum(label_patches, axis=(1, 2, 3)) != 0)\n",
    "\n",
    "        # Filtering extracted patches\n",
    "        label_patches = label_patches[valid_idxs]\n",
    "\n",
    "        x = np.vstack((x, np.zeros((len(label_patches), 2, 27, 27, 27))))\n",
    "        y = np.vstack((y, np.zeros((len(label_patches), 9 * 9 * 9, num_classes))))\n",
    "\n",
    "        for i in range(len(label_patches)):\n",
    "            y[i + y_length, :, :] = np_utils.to_categorical(label_patches[i].flatten(), num_classes)\n",
    "\n",
    "        del label_patches\n",
    "\n",
    "        # Sampling strategy: reject samples which labels are only zeros\n",
    "        T1_train = extract_patches(T1_vols[idx], patch_shape, extraction_step)\n",
    "        x[y_length:, 0, :, :, :] = T1_train[valid_idxs]\n",
    "        del T1_train\n",
    "\n",
    "        # Sampling strategy: reject samples which labels are only zeros\n",
    "        T2_train = extract_patches(T2_vols[idx], patch_shape, extraction_step)\n",
    "        x[y_length:, 1, :, :, :] = T2_train[valid_idxs]\n",
    "        del T2_train\n",
    "    return x, y\n",
    "\n",
    "\n",
    "# Reconstruction utils\n",
    "import itertools\n",
    "\n",
    "\n",
    "def generate_indexes(patch_shape, expected_shape):\n",
    "    ndims = len(patch_shape)\n",
    "\n",
    "    poss_shape = [patch_shape[i + 1] * (expected_shape[i] // patch_shape[i + 1]) for i in range(ndims - 1)]\n",
    "\n",
    "    idxs = [range(patch_shape[i + 1], poss_shape[i] - patch_shape[i + 1], patch_shape[i + 1]) for i in range(ndims - 1)]\n",
    "\n",
    "    return itertools.product(*idxs)\n",
    "\n",
    "\n",
    "def reconstruct_volume(patches, expected_shape):\n",
    "    patch_shape = patches.shape\n",
    "\n",
    "    assert len(patch_shape) - 1 == len(expected_shape)\n",
    "\n",
    "    reconstructed_img = np.zeros(expected_shape)\n",
    "\n",
    "    for count, coord in enumerate(generate_indexes(patch_shape, expected_shape)):\n",
    "        selection = [slice(coord[i], coord[i] + patch_shape[i + 1]) for i in range(len(coord))]\n",
    "        reconstructed_img[selection] = patches[count]\n",
    "\n",
    "    return reconstructed_img\n",
    "\n",
    "\n",
    "T1_vols = np.empty((3, 144, 192, 256))\n",
    "T2_vols = np.empty((3, 144, 192, 256))\n",
    "label_vols = np.empty((3, 144, 192, 256))\n",
    "for case_idx in range(1, 4) :\n",
    "    T1_vols[(case_idx - 1), :, :, :] = read_vol(case_idx, 'T1')\n",
    "    T2_vols[(case_idx - 1), :, :, :] = read_vol(case_idx, 'T2')\n",
    "    label_vols[(case_idx - 1), :, :, :] = read_vol(case_idx, 'label')\n",
    "\n",
    "## Intensity normalisation (zero mean and unit variance)\n",
    "T1_mean = T1_vols.mean()\n",
    "T1_std = T1_vols.std()\n",
    "T1_vols = (T1_vols - T1_mean) / T1_std\n",
    "T2_mean = T2_vols.mean()\n",
    "T2_std = T2_vols.std()\n",
    "T2_vols = (T2_vols - T2_mean) / T2_std\n",
    "\n",
    "# Combine labels of BG and CSF\n",
    "for class_idx in class_mapper :\n",
    "    label_vols[label_vols == class_idx] = class_mapper[class_idx]\n",
    "\n",
    "x_train, y_train = build_set(T1_vols, T2_vols, label_vols, (6, 16, 6))\n",
    "\n",
    "del T1_vols\n",
    "del T2_vols\n",
    "\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Input\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.layers.convolutional import Conv3D\n",
    "from keras.layers.convolutional import Cropping3D\n",
    "from keras.layers.core import Permute\n",
    "from keras.layers.core import Reshape\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model\n",
    "print(np.shape(x_train))\n",
    "print(np.shape(y_train))\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Input\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.layers.convolutional import Conv3D\n",
    "from keras.layers.convolutional import Cropping3D\n",
    "from keras.layers.core import Permute\n",
    "from keras.layers.core import Reshape\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model\n",
    "\n",
    "K.set_image_dim_ordering('th')\n",
    "\n",
    "def generate_model(num_classes) :\n",
    "    init_input = Input((2, 27, 27, 27))\n",
    "\n",
    "    x = Conv3D(1, kernel_size=(3, 3, 3))(init_input)\n",
    "    x = PReLU()(x)\n",
    "    x = Conv3D(1, kernel_size=(3, 3, 3))(x)\n",
    "    x = PReLU()(x)\n",
    "    x = Conv3D(1, kernel_size=(3, 3, 3))(x)\n",
    "    x = PReLU()(x)\n",
    "\n",
    "    y = Conv3D(1, kernel_size=(3, 3, 3))(x)\n",
    "    y = PReLU()(y)\n",
    "    y = Conv3D(1, kernel_size=(3, 3, 3))(y)\n",
    "    y = PReLU()(y)\n",
    "    y = Conv3D(1, kernel_size=(3, 3, 3))(y)\n",
    "    y = PReLU()(y)\n",
    "\n",
    "    z = Conv3D(1, kernel_size=(3, 3, 3))(y)\n",
    "    z = PReLU()(z)\n",
    "    z = Conv3D(1, kernel_size=(3, 3, 3))(z)\n",
    "    z = PReLU()(z)\n",
    "    z = Conv3D(1, kernel_size=(3, 3, 3))(z)\n",
    "    z = PReLU()(z)\n",
    "\n",
    "    x_crop = Cropping3D(cropping=((6, 6), (6, 6), (6, 6)))(x)\n",
    "    y_crop = Cropping3D(cropping=((3, 3), (3, 3), (3, 3)))(y)\n",
    "\n",
    "    concat = concatenate([x_crop, y_crop, z], axis=1)\n",
    "\n",
    "    fc = Conv3D(4, kernel_size=(1, 1, 1))(concat)\n",
    "    fc = PReLU()(fc)\n",
    "    fc = Conv3D(2, kernel_size=(1, 1, 1))(fc)\n",
    "    fc = PReLU()(fc)\n",
    "    fc = Conv3D(3, kernel_size=(1, 1, 1))(fc)\n",
    "    fc = PReLU()(fc)\n",
    "\n",
    "    pred = Conv3D(num_classes, kernel_size=(1, 1, 1))(fc)\n",
    "    pred = PReLU()(pred)\n",
    "    pred = Reshape((num_classes, 9 * 9 * 9))(pred)\n",
    "    pred = Permute((2, 1))(pred)\n",
    "    pred = Activation('softmax')(pred)\n",
    "\n",
    "    model = Model(inputs=init_input, outputs=pred)\n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy',\n",
    "        optimizer='adam',\n",
    "        metrics=['categorical_accuracy'])\n",
    "    return model\n",
    "\n",
    "abc=generate_model(3)\n",
    "\n",
    "#What we have removed so far: images 4 -> 11\n",
    "#patch size that splits into training and test set was doubled: less precision\n",
    "#values from training set after 100\n",
    "x_train=x_train[:100]\n",
    "y_train=y_train[:100]\n",
    "\n",
    "abc.fit(x_train,y_train,verbose=1,validation_split=0.1,epochs=10)\n",
    "#How to use the above NN:\n",
    "#increase number of epochs\n",
    "#increase the number you see right after Conv3D\n",
    "#change optimizer to Nadam, or from the e-mail I sent you\n",
    "'''© 2018 GitHub, Inc.\n",
    "Terms\n",
    "Privacy\n",
    "Security\n",
    "Status\n",
    "Help\n",
    "Contact GitHub\n",
    "API\n",
    "Training\n",
    "Shop\n",
    "Blog\n",
    "About\n",
    "Press h to open a hovercard with more details.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
